{
 "metadata": {
  "name": "",
  "signature": "sha256:a889b3c8ba1c51ea5dbc29a05ec7c104e207cce76fa93da9fed6713f8f625086"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning ex1 (coursera Ng): Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<i>Materials prepared by: <br>\n",
      "Mateusz Zbikowski and Michal Zbikowski<br>\n",
      "Please check the references on which materials where based. </i>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gradient Descent\n",
      "<img src=\"PicturesForTheory/mlANgGradient01.png\"> <br>\n",
      "<img src=\"PicturesForTheory/mlANgGradient02.png\"> <br>\n",
      "<img src=\"PicturesForTheory/mlANgGradient03.png\"> <br>\n",
      "<img src=\"PicturesForTheory/mlANgGradient04.png\"> <br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gradient descent\tintuition\t\n",
      "<img src=\"PicturesForTheory/mlANgGradient05.png\"> <br>\n",
      "<img src=\"PicturesForTheory/mlANgGradient06.png\"> <br>\n",
      "<img src=\"PicturesForTheory/mlANgGradient07.png\"> <br>\n",
      "<img src=\"PicturesForTheory/mlANgGradient08.png\"> <br>\n",
      "<img src=\"PicturesForTheory/mlANgGradient09.png\"> <br>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Check the 01mlTHEORYLinearReg, Gradient Descent in Linear Regersion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Normal equations "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Some notes\n",
      "- fill equatins in latex\n",
      "- edd code\n",
      "- [Gaussian Elimnation](http://en.wikipedia.org/wiki/Gaussian_elimination)\n",
      "- [Cholesky_decomposition](http://en.wikipedia.org/wiki/Cholesky_decomposition )\n",
      "- [Matrix_decomposition](http://en.wikipedia.org/wiki/Matrix_decomposition)\n",
      "- [LU_decomposition](http://en.wikipedia.org/wiki/LU_decomposition)\n",
      "- [Derivatives](http://en.wikipedia.org/wiki/Derivative)\n",
      "- KhanAcademy\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align} J(\\theta) &= \\frac{1}{m} \\sum \\limits_{i=1}^m \\left[ -y_i \\ln h(\\theta_i, x_i) - (1 - y_i) \\ln (1 - h(\\theta_i, x_i)) \\right] \\\\ &= - \\frac{1}{m} \\sum \\limits_i^m \\left[ y_i \\ln h(\\theta_i, x_i) + (1 - y_i) \\ln (\\underbrace{1 - h(\\theta_i, x_i)}_{i(\\theta_i, x_i)}) \\right] \\\\ &\\text{let's drop the sum for now then re-add it later:}\\\\ &= - \\frac{1}{m} \\left[ y \\ln h(\\theta, x) + (1-y) \\ln i(\\theta, x) \\right]\\\\ \\frac{\\partial J}{\\partial \\theta} &= - \\frac{1}{m} \\left[ y \\left(\\frac{1}{h(\\theta, x)}\\right) \\left(\\frac{\\partial h}{\\partial \\theta}\\right) + (1-y) \\left(\\frac{1}{i(\\theta, x)}\\right) \\left(\\frac{\\partial i}{\\partial \\theta}\\right) \\right]\\\\ h(\\theta, x) &= \\frac{1}{1 + e^{-x\\theta}} = \\left(1 + e^{-x\\theta}\\right)^{-1}\\\\ \\frac{\\partial h}{\\partial \\theta} &= -\\left( 1 + e^{-x\\theta} \\right)^{-2} \\cdot e^{-x\\theta} \\cdot -x \\\\ &= \\frac{x e^{-x\\theta}}{\\left( 1 + e^{-x\\theta} \\right)^2} \\\\ i(\\theta, x) &= 1 - \\frac{1}{1 + e^{-x\\theta}} = \\frac{e^{-x\\theta}}{1 + e^{-x\\theta}}\\\\ \\frac{\\partial i}{\\partial \\theta} &= \\frac{\\partial}{\\partial\\theta}(1 - h(\\theta, x)) \\\\ &= -\\frac{\\partial h}{\\partial \\theta} = \\frac{-x e^{-x\\theta}}{\\left(1 + e^{-x\\theta}\\right)^2} \\\\ \\frac{\\partial J}{\\partial\\theta} &= -\\frac{1}{m} \\left[ y \\cdot \\left(1 + e^{-x\\theta}\\right) \\cdot \\frac{x e^{-x\\theta}}{\\left(1 + e^{-x\\theta}\\right)^2} + (1-y) \\cdot \\frac{1 + e^{-x\\theta}}{e^{-x\\theta}} \\cdot \\frac{-x e^{-x\\theta}}{\\left(1 + e^{-x\\theta}\\right)^2} \\right]\\\\ &= -\\frac{1}{m} \\left[ \\frac{y x e^{-x\\theta}}{1 + e^{-x\\theta}} - \\frac{(1 - y)x}{1 + e^{-x\\theta}} \\right]\\\\ &= -\\frac{1}{m} \\left[ x \\left(\\frac{ye^{-x\\theta} + y - 1}{1 + e^{-x\\theta}} \\right) \\right]\\\\ &= -\\frac{1}{m} \\left[ x\\left( \\frac{y\\left(1 + e^{-x\\theta}\\right) - 1}{1 + e^{-x\\theta}} \\right)\\right]\\\\ &= -\\frac{1}{m} \\left[ x\\left(y - \\frac{1}{1 + e^{-x\\theta}}\\right)\\right]\\\\ &= \\frac{1}{m} \\Big(x\\left(h(\\theta, x) - y\\right)\\Big)\\\\ &= \\frac{1}{m} \\sum_i^m \\left(h(\\theta_i, x_i) - y_i\\right)x_i \\end{align}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 3 Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The objective of linear regression is to minimize the cost function\n",
      "$$J(\\theta) = \\frac{1}{2m}\\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
      "\n",
      "where the hypothesis $h_\\theta(x)$ is given by the linear model\n",
      "\n",
      "$$h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1x_1$$\n",
      "\n",
      "One way to do this is with batch gradient descent. Each iteration performs the update:\n",
      "\n",
      "$$\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
      "\n",
      "With each step of gradient descent your parameters $\\theta_j$ come closer to minimizing cost $J(\\theta)$.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# References\n",
      "- [Andrew NG](https://www.coursera.org/course/ml)\n",
      "- [Joe Golton](https://github.com/FilterJoe/machine_learning_Ng_iPythonNotebooks/tree/master/)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}